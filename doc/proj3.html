<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
    "http://www.w3.org/TR/html4/loose.dtd">
<html><head>
<title>Extract Association Rules </title>
<meta http-equiv="Content-type" content="text/html; charset=iso-8859-1">
</head>
<body bgcolor="#FFFFFF">


<p>
------------------------------------------<br>
Readme files for CS6111 Project 3<br>
Group members:<br>
Anuj Arora (aa2583)<br>
Jiacheng Yang (jy2522)<br>
------------------------------------------<br>
<br>
</p>

<h2><b>List of files</b></h2><br>
* source code<br>

<p>

- src/AssociationsFinder.java: Startup and UI module<br>

<p>

- src/data/*: Implementation of schema-driven data pre-processing<br>
- src/data/Schema.java: Represent a database schema <br>
- src/data/FieldInfo.java: Each field/column in the schema <br>
- src/data/SchemaField.java: Enumeration of what fields are included in the schema <br>
- src/data/Database.java: A representation for the entire dataset <br>
- src/data/Record.java: Represent a record in the database <br>
- src/data/Field.java: Represent each field in a record in the database <br>
- src/data/RecordReader.java: An abstract class to read a record from dataset <br>
- src/data/CSVRecordReader.java: Read record from CSV format dataset <br>

<p>

- src/generator/*: Implementation of association rule mining algorithm<br>
- src/generator/DBAccess.java: Wrap the access to database <br>
- src/generator/Itemset.java: Representation of a large itemset <br>
- src/generator/Rule.java: Representation of a rule <br>
- src/generator/ItemsetGenerator.java: implement the Apriori algorithm <br>
 

<br>

* documents<br>
- docs/proj3.html: the readme file to describe our methods<br>
- Makefile: the makefile file<br>
- run.sh: run file<br>
- docs/example-run.txt: An output of our example run<br>

<br>

* data file <br>
- data/boiler.csv: The original dataset, but with several useless columns and erroneous lines removed. <br>
- data/schema.txt: A schema used to parse and pre-process boiler.csv <br>
- data/integrated-dataset.csv: the integrated data file, generated from the above schema on boiler.csv. <br>
- data/*.range: the range boundaries of range-partitioned columns (for internal use) <br>

<br>

<h2><b>How to run our program?</h2></b>

<p>

   * Compile on clic machines <br>
     1) Go into directory association <br>
     2) option1: 'make' <br>
     3) option2: manually compile: <br>
     	javac -cp "src" src/AssociationsFinder.java <br>

</p>

<p>

   * Run tests <br>
     1) Go into directory association <br>
     2) option 1: run script ./run.sh datafile min_supp min_conf<br>
     3) option 2: manually run java <br>
     	java -cp "src" AssociationsFinder datafile min_supp min_conf <br><br>
        
	Example: ./run.sh data/integrated-dataset.csv 0.1 0.7 <br><br>

     4) Output of large itemsets and rules are printed on screen <br>
     5) Output is written to "output.txt" in which also include the range of range-partitioned columns<br>
<br>
</p>
<p>



<h2><b>Dataset Specification</h2></b>


<ol>
<li>About the dataset</li>
We choose a dataset about the oil boilers in New York. This dataset has a wide range of detailed data about each boiler and the building it locates in, for example its location, model, capacity, consumption and so on. The original dataset has 8048 non-null rows and 33 columns in total. You can view our chosen dataset here: <a href=https://data.cityofnewyork.us/Facilities-and-Structures/Oil-Boilers-Detailed-Fuel-Consumption-and-Building/jfzu-yy6n>https://data.cityofnewyork.us/Facilities-and-Structures/Oil-Boilers-Detailed-Fuel-Consumption-and-Building/jfzu-yy6n</a> <br>


<li>Pre-process the dataset</li>
The quality of data is critical for the outcome of our data mining algorithm. We pre-process the data to generate the integrated dataset file. Important operations in this step include removing unnecessary columns, range partitioning numerical values and more. One interesting feature of our pre-processing is that it is based on a user-defined schema similar to the 'CREATE TABLE' clause in SQL. The user defines how to process each column in a simple way, and our pre-processing program will generate the integrated dataset as user wants automatically. <br><br>

<ul>
  <li>Removing uninformative columns</li>
  Firstly, we remove some uninformative and duplicate columns in the dataset. For example, the block id is removed because it doesn't tell us too much about the data. Column 'Natural Gas Utility' is also removed, because every value in the column is of the same value. We remove 17/33 trivial columns that obviously does not help with the association rule discovery from the datasets.<br><br>

  <li>Removing erroneous data</li>
  We also removes erroneous data from the datasets. For example, some boilers have capacities larger than more than 2-3 magnitudes than other boilers. And some rows have almost every informative columns as NULL. But we guarantee that the rows we remove are only of a small proportion of the data (less than 1%). At the end, we keep 7074/8048 rows. <br><br>
  
  <li>Replacing NULL values</li>
  The last thing we do in this step is to padding the NULL value in a column with . In the 'is boiler dual fuel capable' column, only 'DUAL FUEL' is filled in here, while other lines are left blank. We fill in 'Non-dual fuel' for every blank line. Elimination of NULL value simplifies our further processing<br><br>

  <li><b>Schema-driven data cleaning</b></li>
  The highlight of our data pre-processing is a schema-driven data cleaning. In this step, the user provides a schema for the dataset, specifying the format of each column. The schema works like the 'CREATE SQL' clause in database. Our program will generate an integrated dataset according to the schema.<br><br>

  Here is the format for the schema definition: each line in the schema file corresponds to a column in the dataset. For each column, we define the column name(column-name), whether we want to skip that column(skip), what is the type of the column(type), e.g. int, double, string and whether we want to range partition the values. <br><br>
  
  |column-name|skip|type|#partitions| <br><br>

Column name is used to identify a column and it is used to padding the value of a column(will explain later). We can also skip certain columns if they don't help us finding meaningful rules. This is like a feature-selection in machine learning. We also indicates the parser how to interpret the column. Some column may have multiple ways to interpret, for example, year '1999' can either be read as int or as a string.<br><br>

<li><b>Range partition numerical values</b></li>

The last field '#partitions' is especially useful to process numerical values. Numerical values, usually integers and double, come from a continuous domain. We map them to appropriate ranges to discover rules. We use range partitions to pick the range boundaries. For example, if we set the number of partitions to two. Our program will find the median of the column; all values smaller than the median falls into the first partition while all values come after the median go to the second.<br><br>

<li><b>How it works</b></li>
  The <i>DataLoader</i> first reads in the schema file and data file, then it uses <i>CSVRecordReader</i> to parse each line in the data file. After that, some transformation is performed on the data, e.g. range partitioning. Finally, the <i>Database</i>, which contain a list of transformed <i>Record</i> is exported to 'integrated-dataset.csv'.

</ul>
  

</ol>


</ol>

<br><br>

<h2><b>Internal Design</b></h2>
<p>
As listed above, there are two key modules in the project - <br>
<ol>
<li>The high level procedure to generate the integrated-dataset.txt file is described in the section above.   The 'data' module contains the code to generate the intergrated-dataset from the downloaded raw data.  As explained in the previous section, certain items are divided in ranges.  This module generates the applicable range files as well.  The range files are stored in the data folder.</li> 
<br>
<li>The second package 'generator' contains the list of classes required to generate the association rules.  Below is the summary of the classes included in this package and our design decisions - <br>
<br><ul>
<li><b>Itemset.java</b> - This class is the data class to store the itemset.  It stores the items in a TreeSet data structure.  Thus, items are always stored in 'ascending' order based on the name of the item.  This class implements the Comparable interface, which allows the itemsets to be ordered based on the support values.</li><br>
<li><b>Rule.java</b> - This class is the data class to store the rules.  This class implements the Comparable iterface, which allows the rules to be ordered based on the confidence values.</li><br>
<li><b>DBAccess</b> - This class contains the methods to create database connections, generate and execute the queries.  Under default settings, this portion of the code is not run.</li><br>
<li><b>ItemsetGenerator.java</b> - This class is the main processor class.  It contains methods to read the integrated-dataset file, evaluate large datasets, calcuate values of support and confidence, and generate the assocation rules.  This class also contains the method to do a self-join on the large itemsets, as required by the a-priori algorithm.</li>
</ul>
</li>
</ol>
<b>a-priori Algorithm</b> - We implement the algorithm exactly as described in the  Section 2.1 of the Agrawal and Srikant paper in VLDB 1994.  We pre-process the data before running it through the algorithm, so that it can be processed through the algorithm to generate meaningful associations.<br>
<br> 
 Generating Large Itemsets - As mentioned in the algorithm, we have to generate all large itemsets that have support higher than minimum required support.  These itemsets are inspected for high-confidence associations.  The large itemsets, that contains k items are generated using following join - 
<br><br>
SELECT p.item1, p.item2, . . . . p.itemk-1, q.itemk-1<br>
from L_k-1 p, L_k-1 q<br>
where p.item1 = q.item1, . . ., p.itemk-2 = q.itemk-2,<br>
p.itemk-1 < q.itemk-1;<br>
<br>
We provide two alternative methods to implement the above join.  The 'switch' (usesql boolean data member of ItemsetGenerator class) to toggle between these two methods in hard-coded in the code, and is set to use the second method (join is done in the memory).  The second method provides faster speeds, and works really well if the dataset can fit in the memory.
<br>
<ol>
<li>
<b>Using sqlite database</b> - We create a sqlite database table with k columns and do a self-join on that table based on the query above.  The queries to create the table, insert and join the data are built on the fly based on the value k and large itemsets evaluated in (k-1)th iteration.  
</li>
<br>
<li>
<b>Join in the memory</b> - By doing a join in the memory, we store all the itemsets in a List.  These itemsets store all the items in a TreeSet.  We do a self join on this list of Itemsets based on the query above.  By default, the query is executed using this method in our code.
</li>
</ol>
<br>
<b>Optimization</b> - As described in the algorithm, we remove all large itemsets from the large itemsets list if their subsets are not in the list.  
</p>


<br><br>

<h2><b>Example run</b></h2><br>
The command line specification of our example run is: <br><br>
<i>./run.sh integrated-dataset.csv 0.1 0.7</i> <br><br>
The result of this run is shown in "example-run.txt" <br><br>

We find the result in the run interesting for the following reasons:

<ul>
<li>The number of large itemsets and rules are of moderate size</li>
There are 99 large itemsets and 79 rules found in this run. <br><br>


<li>We can derive consistent conclusions from the rules</li>
Note that we want to find interesting rules here, which means the rules are non-trivial. So we have already removed those columns with duplicate information and obvious correlation (e.g. we only keep the age of a boiler and discard its year of installation). So the rules we found here is supposed to be interesting, at least to non-experts in this domain. <br>

(in the following rules, CONSUMPTION, CAPACITY and AREA are range partitioned into 5 parts. 0 is the smallest, 4 is the largest) <br><br>

<ul>
<li>New boilers support dual fuel while old boilers usually do not, and old boilers usually burn #6 fuel</li>
Example rules: <br>
[AGE-5-10] => [DUALFUEL-DUAL FUEL] (Support: 0.1151 , Confidence: 1.0000) <br>
[AGE-26-30] => [DUALFUEL-Not Dual] (Support: 0.1770 , Confidence: 0.8902) <br>
[AGE-26-30] => [PRIMARY-#6] (Support: 0.1432 , Confidence: 0.7200) <br>

<br>

<li>#4 fuel type is used for smaller areas, resulting in lower consumption.  #6 fuel type is user for larger areas, resulting in higher consumption</li>
Example rules: <br>
[CONSUMPTION-0, DUALFUEL-Not Dual] => [PRIMARY-#4] (Support: 0.1081 , Confidence: 0.8011) <br>
[AREA-4, CONSUMPTION-4] => [PRIMARY-#6] (Support: 0.1189 , Confidence: 0.8919) <br>
[CAPACITY-4, CONSUMPTION-4] => [PRIMARY-#6] (Support: 0.1248 , Confidence: 0.8900) <br>
[AREA-0] => [PRIMARY-#4]<br>


<br>

<li>Capacity and the type of fuel are major factors to energy consumption</li>
Example rules: <br>
[CAPACITY-0, PRIMARY-#4] => [CONSUMPTION-0] (Support: 0.1264 , Confidence: 0.7564) <br>
[CAPACITY-4, PRIMARY-#6] => [CONSUMPTION-4] (Support: 0.1248 , Confidence: 0.7271) <br>

<br>

<li>Most smaller area do not have dual fuel boilers installed</li>
Example rules:<br>
[AREA-0] => [DUALFUEL-Not Dual] (Support: 0.1508 , Confidence: 0.7537)<br>

<br>

<li>Higher capacity boilers are needed to serve larger areas</li>
Example rules:<br>
[AREA-4] => [CAPACITY-4] (Support: 0.1449 , Confidence: 0.7234)<br>

<br>

<li>The net fuel consumption by larger boilers is higher</li>
Example rules:<br>
[CAPACITY-4] => [CONSUMPTION-4] (Support: 0.1402 , Confidence: 0.7173)<br>
[CONSUMPTION-4] => [CAPACITY-4] (Support: 0.1402 , Confidence: 0.7004)<br>

<br>

</ul>


</ul>



</body>
</html>
